{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7HCkoZUudSy",
        "outputId": "de548a5d-eeb2-487a-cb81-3bed61fcf356"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import random\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from difflib import SequenceMatcher\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPfFXglbufQ7"
      },
      "source": [
        "start_tiempo= time.process_time()\n",
        "stemmer=LancasterStemmer();\n",
        "\n",
        "with open('intents.json') as file:\n",
        "  data=json.load(file)\n",
        "\n",
        "words=[]\n",
        "labels=[]\n",
        "docs_x=[]\n",
        "docs_y=[]\n",
        "\n",
        "for intent in data['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    wrds=nltk.word_tokenize(pattern)\n",
        "    words.extend(wrds)\n",
        "    docs_x.append(wrds)\n",
        "    docs_y.append(intent['tag'])\n",
        "\n",
        "    if intent['tag'] not in labels:\n",
        "      labels.append(intent['tag'])\n",
        "\n",
        "words=[stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "words=sorted(list(set(words)))\n",
        "labels=sorted(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcSgeP8kukOP"
      },
      "source": [
        "training=[]\n",
        "output=[]\n",
        "\n",
        "out_empty=[0 for _ in range(len(labels))]\n",
        "\n",
        "for x,doc in enumerate(docs_x):\n",
        "  bag=[]\n",
        "  wrds=[stemmer.stem(w.lower()) for w in doc]\n",
        "  for w in words:\n",
        "    if w in wrds:\n",
        "      bag.append(1)\n",
        "    else:\n",
        "      bag.append(0)\n",
        "  output_row=out_empty[:]\n",
        "  output_row[labels.index(docs_y[x])]=1\n",
        "\n",
        "  training.append(bag)\n",
        "  output.append(output_row)\n",
        "\n",
        "training=np.array(training)\n",
        "output=np.array(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxH1x-Ic0oSX"
      },
      "source": [
        "modelo=LinearRegression()\n",
        "\n",
        "modelo.fit(training,output)\n",
        "\n",
        "test=np.random.randint(0,2,size=(1,46))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDbD_jeWXFvc"
      },
      "source": [
        "def bag_of_words(s,words):\n",
        "  bag=[0 for _ in range (len(words))]\n",
        "\n",
        "  s_words=nltk.word_tokenize(s)\n",
        "  s_words=[stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "  for se in s_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w==se:\n",
        "        bag[i]=1\n",
        "  return np.array(bag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o53hL0owGMcD"
      },
      "source": [
        "sujeto=['I', 'We', 'My name', 'It', 'Hello', 'You']\n",
        "verbo=['am', 'have', 'does not', 'is', 'costs', 'sell', 'are', 'good to see', 'can', 'see', 'was']\n",
        "predicado=['18', 'no idea', 'matter','a secret', 'Tim', '20', 'cookies', 'open at 8', 'closed at 20', 'you', 'help you', 'a good day', 'happy to see you', 'you later', 'a pleasure', 'sad']\n",
        "\n",
        "def unirPalabras(vector):\n",
        "  texto=''\n",
        "  for palabra in vector:\n",
        "    texto+=palabra+' '\n",
        "  texto=texto[0:len(texto)-1]\n",
        "  return texto\n",
        "\n",
        "def poblacionInicial(poblacion):\n",
        "  oraciones=[]\n",
        "  for _ in range(poblacion):\n",
        "    oraciones.append([random.choice(sujeto),random.choice(verbo),random.choice(predicado)])\n",
        "  return oraciones\n",
        "\n",
        "def determinarCoherencia(texto):\n",
        "  texto=unirPalabras(texto)\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"}\n",
        "  URL     = 'https://www.google.com/search?q=\"'+texto+'\"'\n",
        "  result = requests.get(URL, headers=headers)    \n",
        "\n",
        "  soup = BeautifulSoup(result.content, 'html.parser')\n",
        "\n",
        "  total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n",
        "  results_num = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n",
        "  print(results_num)\n",
        "  return int(results_num)\n",
        "\n",
        "def mutar_poblacion(oraciones):\n",
        "  oraciones_final=[]\n",
        "  for oracion in oraciones:\n",
        "    oraciones_final.append(mutar_individuo(oracion))\n",
        "  return oraciones_final\n",
        "\n",
        "def mutar_individuo(oracion):\n",
        "  prob=1/len(oracion)\n",
        "\n",
        "  if (prob>random.uniform(0,1)):\n",
        "    oracion[0]=random.choice(sujeto)\n",
        "  if (prob>random.uniform(0,1)):\n",
        "    oracion[1]=random.choice(verbo)\n",
        "  if (prob>random.uniform(0,1)):\n",
        "    oracion[2]=random.choice(predicado)\n",
        "\n",
        "  return oracion\n",
        "  #Funcion para guardar el sujeto con mejor fitness\n",
        "def asignarMejor(actual,nuevo,palabra_actual,palabra_nueva):\n",
        "  if actual>nuevo:\n",
        "    return actual,palabra_actual\n",
        "  else:\n",
        "    return nuevo,palabra_nueva\n",
        "\n",
        "def determinarFitness(respuestas,oracion):\n",
        "  fitness=[]\n",
        "  oracion=unirPalabras(oracion)\n",
        "  for respuesta in respuestas:\n",
        "    ratio = SequenceMatcher(None, oracion, respuesta).ratio()\n",
        "    fitness.append(ratio)\n",
        "  return max(fitness)\n",
        "\n",
        "def respuestasAlgoritmoGenetico(respuestas, poblacion):\n",
        "  iteraciones=0\n",
        "  while (True):\n",
        "    oraciones=poblacionInicial(poblacion)\n",
        "    mejor_fitness=0\n",
        "    mejor_oracion=[]\n",
        "    while mejor_fitness<0.9:\n",
        "      fitness=[]\n",
        "      oraciones=mutar_poblacion(oraciones)\n",
        "      #obtener fitness de cada sujeto:\n",
        "      for oracion in oraciones:\n",
        "        fitness.append(determinarFitness(respuestas,oracion))\n",
        "      indices=np.argsort(fitness)[::-1]\n",
        "      mejor_fitness,mejor_oracion=asignarMejor(mejor_fitness,fitness[indices[0]],mejor_oracion,oraciones[indices[0]])\n",
        "    iteraciones+=1\n",
        "    if (determinarCoherencia(mejor_oracion)>2000000 or iteraciones>=5):      \n",
        "      break\n",
        "  return unirPalabras(mejor_oracion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KpkqX6TbIzp"
      },
      "source": [
        "def chat():\n",
        "  print('Start talking with the bot (type quit to stop)!')\n",
        "  while True:\n",
        "    inp=input('You: ')\n",
        "    if inp.lower()==\"quit\":\n",
        "      break\n",
        "    results=modelo.predict([bag_of_words(inp,words)])[0]\n",
        "    results_index=np.argmax(results)\n",
        "    tag=labels[results_index]\n",
        "\n",
        "    if results[results_index]>0.7:\n",
        "      for tg in data['intents']:\n",
        "        if tg['tag']==tag:\n",
        "          responses=tg['responses']\n",
        "      print(respuestasAlgoritmoGenetico(responses,100))\n",
        "    else:\n",
        "      print('Try again.')\n",
        "    print(\"Tiempo ejecucion = \"+ str(time.process_time() - start_tiempo))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTialH1ScTSY",
        "outputId": "40359f10-a113-4e49-81ce-819554f3151c"
      },
      "source": [
        "chat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start talking with the bot (type quit to stop)!\n",
            "2790000\n",
            "I was happy to see you\n",
            "Tiempo ejecucion = 0.9987652850000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rTpddQYSEp7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}